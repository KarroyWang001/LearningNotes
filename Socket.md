# Scoket与IO多路复用

从`socket`到`服务更多的用户`到`IO多路复`

## socket

可跨主机进程间通信

创建socket的时候，可以指定网络层通信使用IPv4还是IPv6，传输层使用TCP还是UDP

### 以TCP为例

* 服务端
  * 服务端首先调用`socket()`函数，创建网络协议为IPv4，传输协议为TCP的Socket
  * 接着调用`bind()`函数，给这个Socket绑定一个**IP地址**和**端口**
    * 绑定端口的目的：当内核收到TCP报文，通过TCP头里面的端口号，找到对应的应用程序，进行数据传输
    * 绑定IP地址的目的：一台机器有多个网卡，每个网卡都有对应的IP地址，绑定网卡之后，内核收到该网卡上的包，才会进行转发
  * 绑完IP和端口之后，调用`listen()`进行监听，如果需要判断一个服务器中某个网络程序是否启动，可以通过`netstat`来查看对应的端口号是否被监听
  * 服务端进入监听状态后，通过调用`accpet()`函数，从内核获取客户端的连接，如果没有客户端连接，则会阻塞以等待客户端连接的到来

* 客户端

  * 客户端创建好socket之后，调用`connect()`函数发起连接，该函数的参数需要指明服务端的IP地址和端口号，然后开始TCP三次握手
  * TCP连接中，服务器内核实际上为每个Socket维护了两个队列：
    * 一个是还没完全建立连接的队列，称为**TCP半连接队列**，这个队列都是没有完成三次握手的队列，此时服务端处于`syn_rcvd`的状态
    * 另一个是建立连接的队列，称为**TCP全连接队列**，这个队列都是完成了三次握手的连接，此时服务器处于`established`状态
  * 当TCP全连接队列不全为空时，服务端的`accpet()`函数，会从内核的TCP全连接队列中拿出一个已经完成连接的Socket返回应用程序，后续数据传输都用这个scoket。
    * 注意：监听的socket和真正用来传输数据的socket是两个：
      * 一个叫做**监听Socket**
      * 一个叫做**已连接Socket**

* 建立连接之后，客户端和服务端就可以开始互相传输数据了，双方都可以通过`read()`和`write()`函数来读写数据

* TCP协议的Socket程序调用过程结束，整个过程如下：

  <img src="https://pic1.zhimg.com/80/v2-7173d98377518b0b5e78e2a209b60090_720w.jpg?source=1940ef5c" alt="img" style="zoom:50%;" />

  ```
  基于Linux一切皆文件的理念，在内核中Socket也是以[文件]的形式存在的，有对应的文件描述符
  ```

## 如何服务更多的用户

前述TCP Socket调用流程基本只能用于一对一通信，因为其使用的是同步阻塞的方式，当服务端还没有处理完一个客户端网络IO时，或读写操作发生阻塞时，其他客户端是无法与服务端相连的。

需要改进这个网络IO模型，以支持更多的客户端

* 服务端单机理论能连接的最大客户端数
  * TCP连接由<本机IP，本机端口，对端IP，对端端口>四元组唯一确定
  * 服务器作为服务方，一般会固定监听本机的某个端口，等待客户端的请求，因此本机端口和IP是固定的。
  * 对于服务端TCP连接的四元组中，只有对端IP和对端端口是可能变化的，所以最大TCP请求数=**客户端IP数×客户端端口数**
  * 对于IPv4，客户端的IP数最大为$2^{32}$ ，端口数最大为$2^{16}$，于是服务器单机的最大TCP连接数约为$2^{48}$。
* 服务器实际无法承载这个连接量的限制因素：
  * **文件描述符** Socket实际上是一个文件，有对应的文件描述符。在Linux下，单个进程打开的文件描述符数是有限制的，默认值1024，不过可以通过unlimit增大文件描述符的数目
  * **系统内存** 每个TCP连接在内核中都有对应的数据结构，意味着每个连接都会占用一定内存
* C10K问题
  * 如果服务器只有2GB内存，千兆网卡，单机服务器侧能支持1w的客户端并发请求吗？
  * 从硬件资源的角度看，如果每个请求占用不到200kB内存和100kbit的网络带宽，就可以满足1万个并发请求
  * 不过，要想实现真正的C10K服务器，要考虑的地方在于网络IO模型，效率低的模型会加重系统开销，从而会离C10K等目标越来越远

#### 多进程模型

![img](https://pic2.zhimg.com/80/v2-a1da7a141d7fde34c667fdf7906f99ad_720w.jpg?source=1940ef5c)

基于最原始的阻塞网络IO模型，服务器要支持多个客户端，可以使用**多进程模型**，即为每个客户端分配一个进程来处理请求

如上图所示，服务器端存在一个父进程用于监听客户的连接，和多个子进程用于处理和不同客户端的连接

* 主进程监听客户端的连接，与客户端完成连接后，`accept()`返回一个[已连接Socket]
* 主进程通过`fork()`函数创建一个子进程，将父进程的所有相关的东西都复制一份，包括文件描述符，内存地址空间。程序计数器，执行的代码等
* 两个进程刚复制完时几乎一模一样，可以根据**返回值**进行区分，返回值为0，是子进程，返回值是其他整数，是父进程
* 由于子进程会复制**父进程的文件描述符**，于是可以直接使用【已连接的Socket】与客户端进行通信
  * 子进程无需关注【监听Socket】，只需要关心【已连接Socket】
  * 父进程相反，将客户端服务交给子进程来处理，只需要关心【监听Socket】即可

注意：当子进程退出时，实际上内核里还会保存该进程的一些信息，需要做好回收工作，否则会变成**僵尸进程**，随着僵尸进程的增多，会慢慢耗尽系统资源

【父进程】善后【子进程】的方法：

* 调用`wait()`或`waitpid()`函数

多进程方法以应对多个客户端的方式，当客户端数量高达1w时，由于每产生一个进程，必会占用一定的系统资源，且进程间上下文切换的【包袱】很重，性能会大大折扣

* 进程的上下文切换，包含虚拟内存、栈、全局变量等用户空间的资源，还包括内核栈堆、寄存器等内核空间的资源

#### 多线程模型

相比于进程上下文切换的繁琐，可以使用比较轻量级的模型来应对客户端请求——**多线程模型**

线程是运行在进程中的一个“逻辑流”，单进程中可以存在多个线程，同进程的线程可以共享进程的部分资源，如

* 文件描述符
* 进程空间
* 代码
* 全局数据
* 堆
* 共享库等

共享资源在上下文切换时无需切换，而只需要切换线程的私有数据、寄存器等不共享的数据，因此同进程的线程上下文切换的开销比进程间切换要小很多

当服务端与客户端通过TCP建立连接之后，使用`pthread_create()`函数创建线程，将【已连接Socket】的文件描述符传给线程函数，接着在线程里与客户端进行通信，从而达到并发处理的目的

如果每个连接都创建一个线程进行处理，那么当通信结束时，操作系统需要销毁线程，尽管线程切换的上下文开销不大，但频繁创建和切换线程，系统开销依然不可忽视。

可以使用**线程池**的方法来避免线程的频繁创建和销毁

* 所谓【线程池】，是提前创建若干个线程，这样当新连接建立之后，将这个已连接的Socket放进一个队列里，然后线程池里的线程负责从队列中取出已连接的Socket进行处理。

* 流程如下图所示

  ![img](https://pica.zhimg.com/80/v2-d67eb5cc4b947eed8b19846d4ed85cb5_720w.jpg?source=1940ef5c)

* 需要注意的是，这个队列是全局的，每个线程都会进行操作，为避免多线程竞争，在操作这个队列前，要对线程进行加锁

```
上面基于进程和线程的模型，仍是有问题的。每新来一个TCP连接，都需要分配一个新的进程或线程，如果要达到C10K，意味着一台服务器要维护1万个连接，相当于要维护1万个进程/线程，操作系统很难扛得住。
```

#### I/O多路复用

 既然为每个请求维护一个进程或者线程不合适，那有没有可能只用一个进程来维护多个Socket呢？

答案是有的，那就是**I/O多路复用**

<img src="https://pic3.zhimg.com/80/v2-0a86ab90d8167860dec5c695064648f3_720w.jpg?source=1940ef5c" alt="img" style="zoom:50%;" />

尽管一个进程在一个时刻只能处理一个请求，但如果将处理耗时控制在1毫秒以内，这样1秒可以处理上千个请求。把时间拉长来看，多个请求复用了同一个进程，这就是多路复用，这种思想很类似一个CPU并发多个进程，因此也称为时分多路复用。

我们熟悉的select/poll/epoll 是内核提供给用户态的多路复用系统调用，进程可以通过一个系统调用函数从内核中获取多个事件。

select/poll/epoll是如何获取网络事件的呢？

* 先把所有连接（文件描述符）传给内核，
* 再由内核返回产生了事件的连接，
* 然后在用户态处理对应这些连接对应的请求即可。

select/poll/epoll都是多路复用接口

#### select/poll

select 实现多路复用的方法是

* 将已连接的Socket都放到一个文件描述符集合
* 然后调用select函数将文件描述符集合**拷贝**到内核里
* 内核用**遍历文件描述符集合**的方式来 检查是否有网络事件发生
* 当检查到有事件发生之后，将此Socket标记为可读或可写
* 接着将整个文件描述符集合拷贝回用户态里
* 然后用户态再通过遍历的方式找到可读或可写的Socket，再对其进行处理

所以，对于select的这种方式，需要进行2次【遍历】文件描述符集合，一次在内核态，一次在用户态，然后还会发生2次【拷贝】文件描述符集合，先由用户空间传入内核空间，由内核修改后，再传出到用户空间中。

```
注意：select使用固定的BitsMap，表示文件描述符集合，且所支持的文件描述符的个数是有限制的
			在Linux系统中，由于内核中的FD_SETSIZE限制，默认最大值为1024，只能监听0~1023的文件描述符
```

poll不再采用BitsMap来存储所关注的文件描述符，而采用动态数组，以链表的形式来组织，避免了select的文件描述符个数限制，但还是会收到系统文件描述符的限制。

**poll和select存在的共同问题**

* 都是使用**线性结构**存储进程关注的Socket集合，因此都需要遍历文件描述符集合来找到可读或可写的Socket，时间复杂度为**O(n)**
* 它们都需要在用户态和内核态之间拷贝文件描述符集合，这种方式随着并发数的上升，性能损耗呈指数级增长

#### epoll

epoll通过两个方面来解决select和poll的问题：

* epoll在内核态里使用**红黑树**来跟踪进程所有待检测的文件描述符，把需要监听的Socket通过`epoll_ctl()`函数加入内核中的红黑树里，红黑树是一种高效的数据结构，增删查改的复杂度为`O(logn)`，通过对这棵红黑树进行操作，每次只需要传入一个待检测的Socket，减少了内核和用户空间大量的数据拷贝和内存分配。
* epoll在使用事件驱动的机制，内核里维护了一个链表来记录就绪事件
  * 当某个Socket里有事件发生时，通过回调函数，内核即可将其加入到这个就绪事件列表中
  * 当用户调用`epoll_wait()`函数时，只会返回有事件发生的文件描述符的集合，无需像select/poll那样轮训扫描整个Socket集合，大大提高了检测的效率

![img](https://pic2.zhimg.com/80/v2-c7246e74cc64c786360ae12e266e0362_1440w.jpg?source=3af55fa1)

epoll的方式即时监听的Socket数量越来越多的时候，效率不会大幅降低，能够同时监听的Socket的数目也非常的多了，上限为系统定义的进程打开的最大文件描述符个数，因此，**epoll被称为解决C10K问题的利器**

```
注意：epoll_wait 实现的内核代码中调用了__put_user函数，这个函数将数据从内核拷贝到用户空间，因此epoll没有使用共享内存，即只有内核态指向就绪列表，用户态没有
```

epoll支持两种事件触发模式：边缘触发（Edge-triggered) 和 水平触发（Level-triggered）

* 边缘触发模式，当被监控的Socket描述符上有可读事件发生时，服务端只会从`epoll_wait()`中苏醒一次，即使进程没有调用read函数从内存中读取数据，也只苏醒一次，因此边缘触发下，程序需要保证一次性读取完内核缓存中的数据
* 水平触发模式，当被监控的Socket描述符上有可读事件发生时，服务端会不断地从`epoll_wait()`中苏醒，直到内核缓冲区中的数据被read函数读完才结束，目的是告诉我们缓冲区中有数据需要读取

如果使用水平触发模式，当内核通知文件描述符要读写的时候，接下来还可以继续检测它的状态，看它是否依旧可读或可写，因此在收到通知后，无需一次性执行尽可能多的操作

如果使用边缘触发模式，I/O事件发生时，只会通知一次，而且我们也不知道到底能读写多少数据，所以收到通知后需要尽可能地读写数据，以免错失读写数据的机会

* 边缘触发模式下，我们会循环地从文件描述符读写数据，如果文件描述符是阻塞的，没有数据可以读写的时候，进程会阻塞在读写函数处，程序无法继续执行。
* 因此，边缘触发模式一般和非阻塞I/O一起使用，程序会一直执行I/O操作，直到系统调用（i.e., `read()`或`write()`）返回错误，错误类型为`EAGAIN`或`EWOULDBLOCK`。
* 一般来说，边缘触发的效率比水平触发的效率高，因为边缘触发可以减少`epoll_wait()`的系统调用次数，减少上下文切换次数，减少系统调用的开销。

```
注意：I/O多路复用最好与非阻塞I/O搭配使用，因为多路复用 API 返回的事件并不一定可读写的，如果使用阻塞 I/O， 那么在调用 read/write 时则会发生程序阻塞，因此最好搭配非阻塞 I/O，以便应对极少数的特殊情况。
```

